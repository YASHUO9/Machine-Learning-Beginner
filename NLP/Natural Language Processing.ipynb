{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24145399-eeda-469a-a39f-31c3afce019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ea4dc3-c930-4f8a-ad6b-600eafc1612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871e6bd-ff85-4ff6-8063-f5d5f147c096",
   "metadata": {},
   "source": [
    "## Corpus-\n",
    "Body of text ,singular.Corpora is the plural of this .Example :A collection of medical journals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34436b-ed0d-4607-881a-61c8c0aee741",
   "metadata": {},
   "source": [
    "## Lexicon - \n",
    "Words and their meanings.Example English dictionary.Consider,however that various field will have different lexicon "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224a207-43e8-4a4a-8d33-6fec5483d219",
   "metadata": {},
   "source": [
    "## Token -\n",
    "Each \"entity\" that is a part of whatever was split up based on rules.For Example each word is a token when a sentence is \"tokenized\" into words.Each sentence can also be a token if you tokenized out of a paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e077f2c-9635-4ba9-a299-f5e08acb6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "text = \"Hello my friend ,Today is Sunday and I am very Excited about party tonight.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3a68a3-4a6a-4ef8-813a-493a5241755d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello my friend ,Today is Sunday and I am very Excited about party tonight.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6be1f6-dc1a-4043-9e46-4036877af586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baafa07-eab2-406a-a234-1e2292d86ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words -useless data\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332a0a94-32b3-48aa-8d91-43f128ad2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Hello Mam ,I am very much happy to meet you.You are my goal to achieve in this life\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "sentence = [w for w in word_tokens if not w in stop_words ]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fb8a1-5d72-4afb-8b24-97fcfee0fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokens ,\"\\n\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e63ff-01d5-448f-b591-51746cfc333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming words with NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['ride','riding','rides','rider']\n",
    "for i in example_words:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084828d-bc52-4785-8178-c441556736a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming an entire Sentence\n",
    "new_text =\"Hello I am a Machine Learning and Robotic Scientist How can i help you with your research in this field\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for i in words:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56310b8-d37e-4445-ac9e-8b907434a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "print(udhr.raw('English-Latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4cb095f-fca2-432e-8589-76a40d32d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7380259-81f5-4ca5-8d83-794295e70d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e7d3f5-83c3-4068-9b41-3aef6d43b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have some text ,we can train the PunkSentenceTokenizer\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e5e8fb7-6824-450d-9917-f6fd3d8407be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets tokenize the sample text\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc48d3-ec50-4952-b032-8292206ba8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6248a58-6ad7-4da9-a03e-a7213c80c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that  will tag each tokenized word with a part of speech \n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fac141-8768-4343-a475-aaa6dbdd1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf92b3e-daab-4ce2-82b1-688616c7a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1be61-eead-4e3e-a5e1-d9a2258780d5",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or more repetitions.\n",
    ". = Any character except a new line.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be49554-3091-4b2c-9a85-19dab063525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "#Now that we have some text ,we can train the PunkSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "#Now lets tokenize the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "#Define a function that  will tag each tokenized word with a part of speech \n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:2]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #Combine the part of speech tag with a regular expression\n",
    "            chunkGram = r\"\"\"Chunk: { <RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser =  nltk.RegexpParser(chunkGram)\n",
    "            chunked =  chunkParser.parse(tagged)\n",
    "            \n",
    "            \n",
    "            #print the nltk tree\n",
    "            for subtree in chunked.subtrees(filter = lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "            # #draw the chunks with-nltk\n",
    "            # chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a87a9c-00b3-4305-83ec-7b001dfa9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''<RB.?>* = \"0 or more of any tense of adverb ,\" followed by:\n",
    "\n",
    "'''\n",
    "\"\"\"<VB.?>* = \"0 or more of any tense of verb,\" follwed by :\n",
    "'<NNP>*' = One or more proper noun,followed by:\n",
    "'<NN>'? : Zero or one singular noun.\"\"\"\n",
    "        \n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf1e8e-23b7-44b4-9932-62936deeb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chinking with nltk\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "#Now that we have some text ,we can train the PunkSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "#Now lets tokenize the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "#Define a function that  will tag each tokenized word with a part of speech \n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #Combine the part of speech tag with a regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                        }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser =  nltk.RegexpParser(chunkGram)\n",
    "            chunked =  chunkParser.parse(tagged)\n",
    "            \n",
    "            \n",
    "            #print the nltk tree\n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter = lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "            # #draw the chunks with-nltk\n",
    "            # chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059853fe-4936-499f-a061-056af6b24b33",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged,binary = False)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #draw the chunk with nltk\n",
    "            namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa9142-7fca-47d0-820b-a8f81c2d397f",
   "metadata": {},
   "source": [
    "# Text Classification \n",
    "## Text classification using NLTK\n",
    "Now that we have covered the basic of preprocessing for Natural Language Processing  we can move on text classification using machine learning classification algorithms.\n",
    "\n",
    "Previously Covered:\n",
    " 1. Tokenizing \n",
    " 2. Stemming \n",
    " 3. Part of Speech Tagging \n",
    " 4. Chunking/Chinking\n",
    " 5. Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84fe232-3a29-44c0-b0ea-93f965602553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cfbea-68a1-434d-9661-9a70a7c6e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list of document\n",
    "documents = [(list(movie_reviews.words(field)),category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for field in movie_reviews.fileids(category)]\n",
    "#shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "print('Number of Documents :{}'.format(len(documents)))\n",
    "print('First Reviews {}'.format(documents[0]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))\n",
    "print('The word happy :{}'.format(all_words[\"happy\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d489f11-174d-4eb8-9575-e27bcd22f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5242071-39b3-4a32-b673-b8c3befa7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use the 4000 most common words as features\n",
    "word_features = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe250f-e407-4031-93ab-2153f53168cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74c56a-088b-4865-9afc-cb063e35fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a find features function that will determine which of the 4000 word features are contained in a review\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "#lets use an example from a negative movie review\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603409a-dd72-4493-b93b-95677285422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23379d8a-efc5-4740-8b44-da2c76b03248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets do it for all the documents\n",
    "featuresets = [(find_features(rev),category) for (rev ,category) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3398398-320d-4dd7-9632-4e3144a985cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can split the features into training and test data set \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#define a seed for reproducibility\n",
    "random_states  = 1\n",
    "\n",
    "#split the data into training and testing datasets\n",
    "train ,test= train_test_split(featuresets,test_size =.2 ,random_state = random_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93674ea0-3752-4531-a119-de8e6c268398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83e550ba-fb64-4eba-be9e-b00e1149b63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b44b752-41c0-4ce7-a3a2-ecb86aca4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How we use sklearn algorithm in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6456384-c4a2-4a78-a5f1-b4d26ba43943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SklearnClassifier(SVC(kernel = \"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a19d892-07db-4287-9463-dd29fcc71baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(kernel='linear'))>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model on the training data\n",
    "model.train(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64a26db3-1d7f-4a8d-a814-f12d3e25b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy:0.8375\n"
     ]
    }
   ],
   "source": [
    "#test on the testing dataset :\n",
    "accuracy = nltk.classify.accuracy(model,test)\n",
    "print('SVC Accuracy:{}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1c436-3424-4644-a88e-d68ec10f0f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
